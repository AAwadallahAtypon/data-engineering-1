{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99edfaac-0413-459b-bcd6-f08170135b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/dags/assignment_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/dags/assignment_2.py\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "host = \"postgres_storage\"\n",
    "database = \"csv_db\"\n",
    "user = \"aawadallah\"\n",
    "password = \"1234\"\n",
    "port = '5432'\n",
    "\n",
    "\n",
    "engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "def Get_DF_i(Day):\n",
    "    DF_i=None\n",
    "    \n",
    "    try: \n",
    "        URL_Day=f'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/{Day}'\n",
    "        DF_day=pd.read_csv(URL_Day)\n",
    "        DF_day['Day']=Day.split('.')[0]\n",
    "        cond=(DF_day.Country_Region=='Germany')&(DF_day.Province_State=='Berlin')\n",
    "        Selec_columns=['Day','Country_Region', 'Last_Update',\n",
    "          'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active',\n",
    "          'Combined_Key', 'Incident_Rate', 'Case_Fatality_Ratio']\n",
    "        DF_i=DF_day[cond][Selec_columns].reset_index(drop=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return DF_i\n",
    "\n",
    "\n",
    "\n",
    "def _fetch_data_as_DF(**context):\n",
    "    # this to grep all the files names  from the repo \n",
    "    CMD = \"curl -s https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports | grep -Eo '[0-9-]*.csv' | sort -Vu\"\n",
    "    output = subprocess.check_output(CMD, shell=True)\n",
    "    List_of_days = output.decode('utf-8').split('\\n')\n",
    "    List_of_days = [line for line in List_of_days if line.strip() != \"\"]\n",
    "    #Appending all data. \n",
    "    # lst_all_DFs= multiprocessing.Pool().map(Get_DF_i, List_of_days)  I've tried to multiprocce the data but seems like it's not allowed in airflow,\n",
    "    #AssertionError: daemonic processes are not allowed to have children\n",
    "    \n",
    "    lst_all_DFs=[]\n",
    "    for Day in List_of_days:\n",
    "        lst_all_DFs.append(Get_DF_i(Day))\n",
    "    \n",
    "    #ConvertList to DF \n",
    "    DF_all = pd.concat(lst_all_DFs).reset_index(drop=True)\n",
    "    DF_all.to_csv('/home/sharedVol/data.csv')\n",
    "\n",
    "\n",
    "\n",
    "def _minMax_scale_data(**context):\n",
    "    DF_Germany=pd.read_csv('/home/sharedVol/data.csv')\n",
    "    Selec_Columns=['Confirmed','Deaths', 'Recovered', 'Active', 'Incident_Rate','Case_Fatality_Ratio']\n",
    "    DF_Germany_2 = DF_Germany[Selec_Columns]\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    DF_Germany_3 = pd.DataFrame(min_max_scaler.fit_transform(DF_Germany_2),columns=Selec_Columns)\n",
    "    DF_Germany_3.index=DF_Germany_2.index\n",
    "    DF_Germany_3['Day']=DF_Germany.Day\n",
    "    DF_Germany_3.to_csv('/home/sharedVol/Scaleddata.csv')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def _push_data_to_postgress(**context):\n",
    "    DF_Germany=pd.read_csv('/home/sharedVol/data.csv')\n",
    "    DF_Germany_3=pd.read_csv('/home/sharedVol/Scaleddata.csv')\n",
    "    DF_Germany.to_sql('data_without_scaling', engine,if_exists='replace',index=False)\n",
    "    DF_Germany_3.to_sql('data_with_scaling', engine,if_exists='replace',index=False)\n",
    "    \n",
    "\n",
    "\n",
    "def _install_tools():\n",
    "\n",
    "    try:\n",
    "        import psycopg2\n",
    "    except:\n",
    "        subprocess.check_call(['pip', 'install', 'psycopg2-binary'])\n",
    "        import psycopg2\n",
    "\n",
    "    try:\n",
    "        from sqlalchemy import create_engine\n",
    "    except:\n",
    "        subprocess.check_call(['pip', 'install', 'sqlalchemy'])\n",
    "        from sqlalchemy import create_engine\n",
    "        \n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except:\n",
    "        subprocess.check_call(['pip', 'install', 'pandas'])\n",
    "        import pandas as pd\n",
    "        \n",
    "    try:\n",
    "        import matplotlib \n",
    "    except:\n",
    "        subprocess.check_call(['pip', 'install', 'matplotlib'])\n",
    "        import matplotlib\n",
    "        \n",
    "    try:\n",
    "        import sklearn \n",
    "    except:\n",
    "        subprocess.check_call(['pip', 'install', 'sklearn'])\n",
    "        import sklearn        \n",
    "\n",
    "\n",
    "\n",
    "with DAG(\"ETL_JHC\", start_date=datetime(2021, 1, 1),\n",
    "         schedule_interval=\"0 1 * * *\", catchup=False) as dag: #to run it everyday at 1 PM\n",
    "    install_tools = PythonOperator(\n",
    "        task_id=\"install_tools\",\n",
    "        python_callable=_install_tools,\n",
    "        provide_context=True\n",
    "    )\n",
    "    \n",
    "    fetchData = PythonOperator(\n",
    "        task_id=\"fetch_data_and_save_it\",\n",
    "        python_callable=_fetch_data_as_DF,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    minMaxScaleData = PythonOperator(\n",
    "        task_id=\"minMax_Scale_data\",\n",
    "        python_callable=_minMax_scale_data,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    pushDataToPG = PythonOperator(\n",
    "        task_id=\"push_data_to_postgress\",\n",
    "        python_callable=_push_data_to_postgress,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    install_tools >> fetchData >> minMaxScaleData >> pushDataToPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896024e3-eeec-43a0-bf4b-a67286fbd048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb927f-fae7-483c-8503-794d5abb9c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
